//-----------------------------------
// 10.22(금) from 5-2 1p
//-----------------------------------

이전 까지에서 구현한 리스트는 사용하지못함..
메모리 릭이 있음
shared_ptr은 못씀

---- 메모리 릭의 해결방법??
- atomic shared ptr
- stamped pointer -애는 같이 해볼것

-epoch( Epoch Based Reuse)
: lock-free free list를 관리하는 방법
최악의 경우 메모리 릭이 발생
-Hazard Pointer
: 리무브를 하는데 그냥 하는게 아니라
따른 스레드가 지금 쓰고있나 직접 확인하면서 리무브를하는 것
성능은 떨어지지만 epoch랑 다르게 메모리릭이 발생하지않음

안정성 hazard, 성능 epoch
epoch, hazard의 자세한 내용은 학부생 내용이 아니므로 넘김
간단하게만다룬다..

Java?
garbage collection 이 해줌..
단 가비지 컬렉션 해주는 언어는 성능이 구짐
-----------------------------------------------------------------------------
Epoch..
별도의 카운터를 추가
공유객체를 누군가가 사용할때(add, remove를 호출할 때) 해당 카운터가 증가

- 메모리 풀을 사용,, 
delete 된 객체들을 delete list를 사용해서 delete해도 crash가 나지않도록 관리한다
그대로 두면 메모리릭이 심하므로
일정 시간이 지나면 delete_list를 비움
단, 아무생각없이 지우는게 아니라 epoch counter를  비교해서 모든 쓰레드 중에서 
epoch counter보다 작은 값을 갖는 객체만 삭제하도록 한다.

치명적인 단점..?
어떤 스레드가 메소드 호출 후, 정상 동작 하지 않아 epoch counter가 증가하지않은경우
메모리 릭이 발생할 수 있음

----------------------------------------------------------
shared_ptr의 문제점 
- 느리다...
왜 느린가?
일반적인 포인터는 pred = &curr,, 단순히 int 계산과 같음, 그대로 복사해오는것
그런데 shared_ptr은 주소의 복사뿐만 아니라 reference counter까지 관리를 해줘야 함
1. ref ++(wait-free)
2. ref--(wait-free)
3. memcpy(~~~) 형식 (non-atomic)

- atomice 문제
레퍼런스 카운터는 아토믹한데(심지어 wait-free), 포인터 자체의 접근은 아토믹하지 않음
만약 1,2,3이 각각 아토믹하다고 하더라도 전체 과정이 아토믹한것은 아님

atomic_shared_ptr은 아직 표준은 아님

atomic한 명령들을 사용해서 shared_ptr을 atomic하게 사용할수도 있음
pred->next = curr->next 형식에서
atomic_excahnge(&pred->next, atomic_load(&curr->next)처럼 사용하게 됨
각각의 명령들이 글로벌하게 lock-unlock을 걸기에 성능은 더 떨어짐
경우에 맞춰서 사용하는 경우(계정 로그인,로그아웃)도 있긴하지만 shared_ptr 어지간하면 쓰지말자...

--------------------------------------------------------------------------------------
여기서부터 새 챕터(5-2. 배경이론)

?합의 객체?
... Non-blocking 알고리즘을 만들기 위해 필요한 객체
합의수
-non-blocking 알고리즘을 만드는 능력
크면 만들수있고, 작으면 만들수 없음
만능성
- 모든 알고리즘을 wait-free하게 만들수 있는가


atomic?
기본 자료형에 대해서는
atomic<~~>을 사용하여 atomic연산을 수행하고, wait-free로 수행한다.

struct{int x,y,z;} 이정도 간단한 자료형에 대해서는 atomic하게 동작함
단, wait-free라는 보장은 없음
내부적으로 mutex를 사용하여 동작하기 때문
성능이 꾸지다는것

단 복잡한 자료구조 ex) vector등에 대해서는 적용할 수 없음
만능이 아니다...


동기화?
자료구조의 동작을 atomic하게 구현하는 알고리즘
(성긴, 세밀한, 낙천적인, 게으른, 논블로킹)
동기화를 구현하기 위해서는 기본 동기화 연산들을 적절하게 사용해야 함
(load, store, atomic_thread_fence)..각각은 wait-free 연산임
lock-unlock은 blocking연산임,, 그러니 이는 제외하고 위의 3개를 잘 스까서 사용하기
------------------------------------------------------------

증명은 어떻게?
직접 비교가 어렵기에
중간다리 개념인 합의객체를 도입하여 증명하는 방법을 사용

decide라는 동기화 연산을 가지고 있는 객체를 합의객체라고 한다

decide의 동작은? type_t decide(type_t value)
n개의 쓰레드가 decide를 호출해도 
atomic하게 동작을 하고
한쓰레드가 한번이하로만 호출(증명을 위해서 이렇게 가정,)
모든 쓰레드에서 호출했을때 동일한 결과값을 도출해야함
쓰레드가 value로 전달한 인자 중에서 값을 반환
기본 동기화 연산이기에 wait-free하고 atomic하게 동작을 해야한다.

--- 모든 쓰레드가 동일한 결과를 얻을 수 있도록 하는 방법
CAS를 사용하지 않고 합의객체를 만들수있지않느냐?
load, store를 사용해서 ...

이를 위해서 합의 수라는 개념을 설명
클래스 C를 사용해서 합의객체를 구현할 때
몇개스레드에서 합의 객체를 구현할 수 있는지를 나타내는 수

가장 처음 예시는 합의 수 1, 싱글 쓰레드에서만 가능
CAS를 사용한 것은 무한대, 쓰레드 개수에 상관없이 합의 가능

return 값에 영향을 미치는 연산은 오로지 read/write(atomic_load / store)
로컬변수에 대한 연산은 반환 결과에 중요하지않음

공유메모리 접근에 대해서만 떼어내서 ( 알고리즘의 모든 실행 가능한 경로를 이진 트리로 나타냄)
이진트리로 나타내고 이를 프토로콜이라고 부른다
트리의 높이는 정해져있음
트리의 결과를 보고 알고리즘이 맞고 틀리고를 판단가능(리프 노드의 값을 보고)

~~~ 
atomic 메모리로 2개 쓰레드 합의객체를 구현할 수 없음을 증명함
합의수 1짜리로는 합의수 2짜리를 만들 수 없음...
--> atomic 메모리 연산(합의수 1)만으로는 atomic queue 못만듬(합의수 적어도2 이상)

//-----------------------------------
// 10.26(목) from 5-2 32p
//-----------------------------------

다중 대입객체는 넘 비용이 크기에 대안으로 RMW연산 사용
read / 원래있던 값을
modify / 수정
write / 한 값을 쓰는 연산
하드웨어적인 지원이 필요
getAndAdd(~~) // C++ 공식함수는 fetchAndSet(~~)

getAndSet... 한 함수가 다른 함수를 덮어쓰는 경우
getAndIncrement()... 호출 순서가 달라져도 값이 같은 경우

최근의 cpu에서는 Common2 RMW연산을 지원하지 않는 추세..
x86은 79년도에 나온 구식 형식에서 출발하였기에 해당 연산을 지원
(sum++ 연산? getAndIncrement에 해당)

50p...
atomic 메모리 read/write만을 사용해서 
모든 자료구조를 구현할수 없는 것은 아님, 가능한 자료구조들도 있긴하다는것
CAS를 쓰면??
->계속해서 배울 내용

"만능" ??
- 모든 알고리즘을 non-blocking으로 만들수있는경우

무한대 수를 합의할 수 있는 객체는?
- CAS..
- 메모리 이동(copy), 수업시간에 다루지않음, 이 방법으로 구현된 cpu가 없음
- LL-SC, 실제 사용되고 있는 연산임
-- load linked, Store Conditional
-- 해당 내용은 뒤에서 배울 것, 모바일 기기에서 사용되는 cpu에서 지원
-- arm 에서는 기계어로 구현된 CAS연산이 없고 대신 LL-SC를 사용하는 것
-- C++11에서는 CAS만을 쓰지않느냐?
-- LL-SC를 사용해서 cas를 구현할 수 있다

56P 만능의 정의
- 클래스 C 객체..?? (합의 객체)
-

조건 A가 결정적이다?
- 같은 값(파라메터)으로 함수를 수회 호출했을 때, 같은 결과를 반환

순차객체(Sequential Object)
- 병렬화 하고자 하는 객체를 감싸서 통일시킨 형태로 객체를 표현
- 구현의 편의성 향상

Log
- 결정적이다?
-- Object a가 있을 때, a는 초기상태 or 최종상태 or 중간상태를 가질것
-- a의 상태를 복사하려면?
-- 1. 모든 member값 복사
-- 2. a의 초기상태부터 현재상태까지에 이르는 함수를 순차 호출
---- 호출된 메서드의 리스트를 Log라고 표현.. 이는 2번 방식


62P
-head는 배열로 관리함
-- 배열의 크기는 쓰레드 개수만큼, 각 쓰레드에서 데이터 레이스 회피용
-- apply함수에서 적어도 매 스탭마다 하나의 쓰레드에서는 작업을 진행하니 lock-free
-- 다른 스레드가 무엇을 해주기를 기다리지않음


//-----------------------------------
// 10.27(금)
//-----------------------------------
LockFree가 아니라 WaitFree로 변경하려면?
decide부분을 기존 lockfree알고리즘에서 변경
- starvation 해소를 위해 실패횟수를 계산하여 decide에서 지더라도
- 다른 스레드가 실패가 누적된 스레드의 작업을 대신 수행
--> 정해진 시간안에 모든 스레드가 작업을 완료
- 자세한 내용은 생략하심...
- 간단하지는 않지만 궁금한 사람들은 교재 보라~~
- 자세한 증명은 대학원 과정

- 79P 이후 수업은 생략하심

6장. 병렬알고리즘 - Queue

Pool
- 풀의 길이 제한이 있는 경우가
성능이 좋은 경우가 많음.
- 구현하기에도 쉬움
- 단, 제한이 있다는점이 치명적일 수 있기에
- 무제한 큐를 사용하는 경우도 많음

- 메소드의 성질
-- 완전
-- 부분적 .. blocking
-- 동기적.. data race가 없고, locking 신경쓸 필요가 없고, 쓰레드 개수가 많은경우
 동시다발적으로 처리하기에 오히려 성능이 높아질 수 있음
-- 우리는 무제한 완전 큐를 구현하는 것을 목표로 한다.

6P, conditionVarialble을 사용하는가?
- blocking이고 운영체제 호출을 하면서 동작하기에 고성능이 아님
- 사용하지 않는다...


16P
- if문 이후에 tail = e 사이에 다른 쓰레드가 작업을 하면 문제가 생기지 않느냐?
- if문에서 cas가 성공을 했다면 오동작하지 않음
- 다만 blocking일 뿐임
- tail = e가 될 때(업데이트될때)까지, 다른 쓰레드는 if문을 성공하지 못하므로


if (nullptr != tail->next) {
	tail = tail->next;
}의 문제점?
- 다른 스레드가 실컷 작업해둔 쓰레드를 
- 다른 쓰레드가 조작하여 결과가 사라질 수 있음
- 비단 deq뿐만 아니라, enq의 상황도 고려하면 제대로 동작하지않음

22P까지의 Deq로직에서도 문제가 있음..
- 


//-----------------------------------
// 11.04(목)
//-----------------------------------
중간고사
1. 다른 쓰레드의 Stack메모리에접근할 경우 운영체제가 예외상황을 발생시킨다.(X)
2. 메모리 일관성 문제는 다른 코어에서 실행하는 쓰레드에서 관찰되는 문제이기 떄문이다.
또한 싱글 코어 내부에서는 메모리 일관성 문제를 해결하는 HW가 존재하기 때문이다.

5.C) X... 중간값 문제가 언제 발생하는가? int의 주소가 4의 배수가 아닌경우에 발생한다.
 D) X

 7? 이유는 victim이 아토믹이 아니기 때문에 메모리 접근 역전이 일어날 수 있기 때문이다.

 8
 Node* p = head->next;
 while(p->value <= pred->value){
	if(p==pred){
		return pred->next == curr;
	}
	p = p->next;
 }

 9-1)
 int t = next;
 *removed = 1 ==(next &1);
 
 b_ A pred->cas_NEXT(curr, succ, false, false);

이전의 성긴동기화 Queue, LFQueue의 성능이 좋은가?
싱글스레드보다도 빠르지않다...
자료구조 자체의 문제...
쓰레드가 늘어나면 늘어날수록 동시다발적으로 여러곳에 접근하는 것이 아니라
head, tail에만 접근하므로 bottle neck현상... 
자료구조 자체가 병렬성이 떨어진다.
그래도 convoing을 막기 위해서라도 락프리 큐를 사용하는 것이 좋다.

드물게 crash...
생각해볼수 있는 문제?
compiler
memory ordering
중간값
ABA... 

ABA 
해결책1)
HW가 32bit, 64bit 127bit cas를 지원한다...
LL, SC 기계어 명령을 써서?
- LL,SC는 값을 보지 않고 다른 쓰레드가 해당 메모리를 건드렸는지를 통해서 파악
(dirty bit를 조사하는 방식으로)
- ABA문제는 하드웨어로 구현된 CAS를 사용할 때 발생하는 문제이다.
- 단, x86 CPU는 구형 CPU이기 떄문에 LL, SC가 없음
- 최신 CPU는 ABA문제로부터 자유롭다.
- 성능은 CAS를 사용하는 경우가 더 좋다.
- LL, SC에서는 가짜 충돌이 발생할 수 있음

2) shared_ptr를 사용
- 성능에 상관없으면 쓰면 되지만,
 성능이 중요하다면 사용하면 안된다...
- Java와 같은 gargabe collection을 사용하는 언어에서는
ABA문제가 발생하지 않는다.
- 단, Java자체가 C++에 비해서 속도가 빠르지 않기에 ...
Java로 갈빠에야 차라리 C++에서 성긴동기화를 사용할정도

3) 별도의 메모리 관리기법을 사용하는 경우
hazardPointer? 학부레벨 아님
EBR - eclass에 참고코드 올려주심

Time Stamp Version??

//-----------------------------------
// 11.05(금)
//-----------------------------------
7장 시작 - STACk, SKIP-LIST

우리 강의에서는 map은 다루지 않음..
stl의 set을 병렬알고리즘으로 구현하는 과제를 생각해보면, map도 가능함을 생각해볼것
hash_map도 생략... 학부레벨에서 해보면 좋지만 시간 관계상 생략

- 시작은 동일하게 무제한 성긴 동기화부터
- stack의 구현은 어떻게?
-- 연결리스트, 보초노드 없이 top 필드 사용

- 병렬 알고리즘 프로그램에서 CAS를 사용할 때,
- 변수 사용을 조심해서 할 것.
- 변경하고자 하는 변수를 직접적으로 사용하지 않고 변수에 담아서 사용해야
- 해당 값이 결정된 상태로 CAS를 진행할 수 있음

- 왜 LF_STACK에서 싱글스레드 알고리즘 stack을 실행한것 보다 성능이 좋게 나오는가(1개 코어에서)...?
- 의문만 제시하고 넘어가심...
- 아마 프로그램이 실행될 때의 클럭속도가 차이가 있을것,, 터보모드 등에 의해서

- [ CAS overhead ]
- CAS가 단순히 쓰레드 혽자서 실패하면 해당 쓰레드만의 실패로 끝나지않는다...
- 메모리 버스를 locking을 하고, 또 cas에 사용되는 메모리 주소에 대한 캐시 동기화가 같이 실행이 된다.
- 그래서 실패한 쓰레드가 다른 쓰레드의 실행에도 성능 저하를 일으킨다.
- 실패가 전체적인 프로그램의 성능에 영향을 줄 수 있고, 코어가 늘어나면 늘어날수록 해당 효과가 크게 나타날 수 있다.

- 이를 줄이기 위해서? BackOff
- CAS에 대기시간을 주는 개념
- 실패했을 경우 다시 시도해도 실패할 확률이 큼
- 그러니 바로 시도를 하는 게 아니라
- 적절한 시간이 경과 후에 시도를 하도록 하는 것

- 그렇다면.. 적절한 시간이란??
- 이번 CAS와 다음 CAS실행 까지의 시간,  
- 몇 개의 쓰레드가 동작하는지(전체 쓰레드 개수X, 현재 스택에 접근하는 쓰레드 개수 O)
- 쓰레드마다 다른 시간 간격을 부여해야 함
- 고정적인 시간보다 adapted하게, 짧게 쉬었다가 반복 실패 시 쉬는 시간을 길게 갖도록
- CAS를 대기 시간이 다 끝나고 실행하는 것이 아니라
- 대기 시간안에 랜덤하고 균일하게 실행하도록 해서 충돌을 피하도록 한다.

- 21P의 InterruptedException()코드는 사용할 수 없음..
- microseconds라는 단위가 너무 큼 (1ms = 1000 ns)
- sleep_for는 운영체제 호출

//-----------------------------------
// 11.11(목)
//-----------------------------------

BackOff 스택(단순 LFBO)을 적용했을 때,
쓰레드 개수가 많아짐에 따라 유의미한 성능향상을 보임...
근데 이걸 쓰면 문제 가 있음
왜 성능이 좋아지는가?
- 충돌이 줄어듬
- 그건 맞는데 딜레이가 엄청 큼
- 딜레이?
- 원래라면 충돌해도 잠깐 쉬었다가 다시 복귀 해서 작업
- BackOff 를 쓰면 왕창 쉬었다가(시스템콜, ms) 복귀하게 되는 구조
- 실패했을 때 패널티가 너무 큼
- 싱글스레드로 작업하는 것처럼 돌아가므로 싱글스레드 만큼의 속도가 계속 나오는 결과를 보여준것

- 시스템콜을 쓰지말고 BF를 구현하면??
- 어셈블리를 사용하는 방법
- TimeStampRegisterCounter(_asm RDTSC)
- 멀티코어 환경에서 코어마다 RDTSC를 따로 가지고 그에따라 각각의 코어마다 시간이 다른 문제가 생김..
- 0번코어에서 실행되다가 1번코어로 옮겨서 작업하면 값이 달라짐
- 인텔이 변경
- 메인보드에서 읽어오게 변경
- 그래서... 정밀한 시간값이 아니게 됨, 메모리 읽어오는 연산이므로 비싼 연산
- 아무작업도 안하고 루프를 반복하는데 성능 저하를 일으킴

- 그래서 RDTSC를 사용하지 않고 다른 어셈블리를 사용하는 방식으로 구조를 또 변경
- 이 방식에서는 루프에서 메모리를 아예 접근하지 않음
- BackOff의 성능은 코어가 많은경우에서 더 확연하게 들어남

- 소거??
- 3개의 상태
Empty : 쓰레드가 첫번쨰로 온경우 -> Waiting으로 변경
Waiting : 한 개의 쓰레드가 대기중인 경우
Busy : 이미 다른 쓰레드들이 대기중인 경우

CAS를 가지고 slot의 상태(위의 3개)를 비교하면서 작업을 수행한다

교환자 알고리즘에서는 cas를 실패하는게 바람직한 경우가 되는 특이한 알고리즘

ABA문제가 생기는가?
- 초기에 쓰레드가 도착했을 때 WAITING,
- 다른 쓰레드가 작업을 해서 WAITING - busy - EMPTY - WAITING의 상태로 돌아온 경우

//-----------------------------------
// 11.12(금)
//-----------------------------------
- SkipList?
링크드리스트의 개선판이라 생각하자

- 이전까지 실습에서 구현한 자료구조는 링크드리스트를 사용
- 검색시간이 O(n)
- 실제로는 자료구조로 검색속도로 O(log(n))의 속도를 갖는 자료구조들을 사용

- 랜덤자료구조?
- Input값의 결과로 변경되는 자료구조의 모양이 랜덤하게...
- 같은 input값을 넣었다고 꼭 같은 결과가 나오라는 보장이 없음
- WorstCase :: O(n)의 속도
- 그럼 왜 씀??
- 어떤 결과를 자료구조에 넣어도 worstcase가 나올 확률이 매우 낮음..무시해도 될 정도
- 천문학적으로 낮은 수치

- 훌륭한 이유는 ?
- 리스트의 길이가 길어져도 성능저하가 작음
- 앞으로 할것?
- skiplist 를 LockFree로 만들자..
- 단, 한번에 갈 수는 없고 한단계(lazy)를 거쳐서 간다

- 게으른 동기화에서 add가 문제가 된다.
- skiplist에서 add과정은 atomic이 아님
- 왜? 한번에 연결되는 것이 아니라, 레벨에 따른 중간상태가 존재하기 때문에

//-----------------------------------
// 11.19(금)
//-----------------------------------
7장 101P
삭제 시에 "모든"  노드의 next에 표시, 0번 층에만 표시하는 것이 아님
Find에서 순회하면서 삭제할 때, 본인의 층에 있는 노드에 대해서만 삭제를 함

102P
모든 Next 포인터를 마킹된 합성 포인터 자료구조로 사용
fully linked 없음
- 해당 개념까지 넣어서 프로그래밍하려면 너무 복잡해짐
- 뺴고 프로그래밍

103P
- 찾은 층수 리턴안함, 검색 성공 여부만 반환
- 어떤 노드가 리스트에 연결되었는 지 판단 방법?
---- 0층에 존재한다면 존재하는 것
- ex) 3레벨에서 찾았더라도 0레벨에 없으면 없는 것으로 판단하여 false반환

- remove에서는 마킹에 실패했더라도 계속 진행해야함
- 왜 실패?? 다른 쓰레드랑 동시에 remove

- Find에서 노드를 제거하는 것이 아니라, 링크를 지우는 것

- contains 는 wait-free,
- 상용프로그램에서는 contains의 호출 빈도가 70% 정도,
- contains에 맞추어 최적화 한 결과...

//-----------------------------------
// 11.25(목)
//-----------------------------------
C++11
Condition variable 정도만 고려해볼 만, 나머지 별로

OpenMP (open Multi Programming)
쓸만한 물건은 아님,,,
왜?
컴파일러 디렉티브? C언어랑은 관련이 없는 별도의..
- 컴파일러에게 명령을 내려 속성을 정해주는, 
- ex) pragma~~~와 유사
- OpenMP 컴파일러를 지원해야지만 OpenMP를 쓸 수 있음
- visual studio, gcc는 지원함

분산 메모리?
CPU가 서로 독립적으로 작업을 수행하며 상대방의 메모리를 읽을 수 없는 경우

- OpenMP의 특징 13P 참조
- 지금까지 실습하면서 멀티쓰레드 프로그래밍에서 경험했던 문제들이 그대로 존재,,
메모리 일관성, aba 등등
- FLUSH == atomic_thread_fence

TBB
- TBB의 메모리 일관성 지시도 역시 atomic_thread_fence와 같은 내용이기에  C++11과 통합하며 지원에서 제거된것,
- 2015년도부터, visual studio는 TBB의 고성능 메모리 할당자를 사용하므로 차이가 없으나
다른 컴파일러 환경에서는 적용되지 않을 수 있으니 아직 남아있는 것


//-----------------------------------
// 11.26(금)
//-----------------------------------

loop 병렬화를 TBB를 써서 가능, 단 게임과는 맞지않으나 세상에 게임만이 어플리케이션이 존재하는 것은 아니지않나
- Open MP에서도 존재함, Open MP의 개발 목적이기도 함

1.. for문의 내용을 함수로 패키징
2.. 패키징한 함수를 클래스화, 함수객체.. operator() 오버로딩(오버라이딩?)
3.. TBB 함수 호출, parallel_for(~~~~~)

... 너무 귀찮지 않음??
... 병렬화해서 많이 빨라지긴 하는지??
.. 그런데
... TBB가 발전 ... > 그냥 람다 쓰자

... parallel for를 쓴다고 해서 메모리 일관성 문제나 기타 등등을 해결해주는 것은 아님에 주의


recursive_mutex는 tbb에서 먼저 사용, 원조까지는 아니더라도, 이걸 C++에서도 차용해서 사용하기 시작한것

RW lock??
... 세밀한 동기화를 구현한다고 할 때,
while(cur_value < x){
	~~.lock()

	pred = curr;
	curr = curr->next 
	...

	~~.unlock()
}
처럼 구현을 했었음, 이는 엄청난 오버헤드, 병렬성이 떨어짐
병렬성을 위해서 다른 쓰레드가 read만 한다면 동시에 read해도 상관없지 않느냐?? 하는 개념



//-----------------------------------
// 12.02(목)
//-----------------------------------

RW Lock, Read lock - Write lock
TBB 에서 mutex_lock 은 C++11에 기능을 넘김
단 RW_LOCK은 TBB에 있는 내용을 사용해서 실습을 할 것, 최근 C++14에서는 존재하긴함

상용프로그램에서는 검색(read_only)이 보통 70%이상의 점유율을 가짐
왜 성능이 낮은가?
contains, add, remove를 같은 비율로 실행하기에
성능향상이 크게 보이지 않는것
contains의 호출 비율을 높여 실습

실습 결과, 성능이 좋아지긴 함
단 해당 결과는 contains함수 자체가 add, remove보다 빠른 함수이기에 나타난 결과
60% read가지고는 유의미한 결과가 나오지 않았다는 것..
... 멀티쓰레드 프로그래밍의 결과로 병렬성이 향상 되지 않음
... Benchmark를 수정해본바(40만회-range 4000), RW_LOCK이 성능향상에 효과는 있다.
shared_mutex가 만능은 아니지만, 경우에 따라서 엄청난 성능을 얻을 수 있다...
잘 써라...
... C++11에 있는 것 보다 직접 구현해서 사용하는 것이 성능은 더 좋게 나올 것

메모리 할당자?
60P의 내용은 옛날 이야기, 스킵
왜? 비쥬얼 스튜디오에서 TBB의 메모리 할당자를 받아들여서 사용하기 떄문이다

... static 스케쥴링
| | | |
| | | |
|   | 
|   
코어마다 실행속도가 틀림,
특정 코어가 실행시간이 딜레이 되면 전체 실행시간이 가장 긴 수행시간에 맞춰 늦게 종료가 되게 되고
나머지 코어는 특정 코어가 딜레이 된 시간만큼 작업하는 동안 놀게 된다.

... dynammic 스케쥴링
거의 동시에 작업에 끝나게 되고 노는 코어가 없게 됨
1억 만들기의 경우 dynamic 스케쥴링이 필요가 없지만
일반적으로는 필요

직접 구현하기 어떤가?
귀찮다... 어렵다...
intel이 이를 구현해둔 것이 이러한 테스크 스케쥴링을 구현 해둠
--> Parallel_for

CUDA ?
... 
핵심은 GPU에서 대규모 병렬처리를 수행한다는 것
속도가 몇백배 빠름 / CPU보다
GPGPU API중 하나, OpenCL, DirectCompute 등

CUDA는 nvdia 그래픽카드 환경
DirectCompute는 윈도우 환경
OpenCL은 환경 안가림,
단 CUDA를 대부분 사용... 왜 ? 시장 선점

GPU는 master slave 방식으로 동작

//-----------------------------------
// 12.03(금)
//-----------------------------------
cuda도 역시 멀티쓰레드 프로그래밍 방식이다.

cuda를 써서 1억만들기를 왜 안하냐?
코어가 몇백, 몇천개가 있든 한번에 하나만 sum에 접근하기에 삽질이다...

local_sum방식으로 접근하면 되지 않느냐?
결국 sum += local_sum에 코어 수만큼의 접근이 필요해서 똑같은 삽질
...하나도 안빠름

mutex를 쓸때의 문제점 
.. priority inversion, convoying
.. deadlock,, deadlock의 4대 요소 중 하나 이상을 제거해서 파회 가능히디
...... 4대요소??
......

유일하게 했던 증명?? 합의... atomic_read_wirte만으로는 non-blocking 만들수없다는 것

소프트웨어 트랜잭션 메모리의 경우 
구현상의 오버헤드로 인하여 하드웨어 트랜잭션 메모리에 비해서 성능이 좋지않다...

트랜잭션 실행중 : active
충돌이 없어서 메모리에 쓰는 중 : commit
충돌이 있어서 undo or redo 중 : aborted

하드웨어 트랜잭션 메모리의 경우
충돌이 생기면 그 즉시 작업 수행을 중지...
이는 소프트웨어 트랜잭션 메모리의 zombie 트랜잭션 문제를 발생시킺 ㅣ않음...

L1 캐시의 용량한도??

트랜잭션을 실패하면,
캐시를 비우고 프로그램 카운터를 작업을 수행하기 전 주소로 복귀하면 된다...
+ 메모리뿐만 아니라 레지스터의 변경까지 roll-back

성긴동기화가 아니라 게으른 동기화로 하는 이유가 무엇이냐??
트랜잭션이 add의 초기에 삽입이 되는 경우,
while루프를 돌면서 node를 찾는 과정에서 메모리에 들어가는 내용이 너무 커져서
L1 캐시에 안들어갈 가능성이 높아짐...
그래서 트랜잭션 메모리로 돌아가는 구간이 최대한 짧은 것이 좋음


//-----------------------------------
// 12.09(목)
//-----------------------------------
충돌이 많을 때 transaction memory의 성능은??

게으른 동기화보다는 빠른데... lock-free보다는 느리다

클라이언트?
CPU에 맞춰서 하드웨어 트랜잭션 메모리를 지원하면 쓰고
Lock-Free를 쓸 수 있으면 Lock-Free가 베스트
못쓰면? 뮤텍스..

서버?
서버 개발자가 CPU를 골라서 구매할 것이기에
무조건 가능하다라고 생각가능

서버 쪽 개발 흐름?
(1.mutex로 개발) -> 2.하드웨어 트랜잭션 메모리로 변경 -> 3.바틀넥 걸리는 부분을 Lock-Free로 변경

생산성
108 P
Single Thread 알고리즘을 그대로 사용할 수 없음
+ 성능 향상을 위해서 접근하는 공유메모리의 개수를 줄여야함
+ 성능향상을 위해 트랜잭션의 실행 시간을 단축시켜야 함
+ 시스템 호출 제거

Nested Transation이 가능은 하지만, child에서 실패하면 자기 선에서 처리할 수 없고
모든 작업은 모두 Roll-Back해야 함

tbb... 
최신 사용방법??
ppt에 안넣으심
사용법
#include tbb/task_grou.h
tbb::task_group<객체명>

run();
run_and_wati();
wait();
cancel()
is_canceling() : cancel이 진행중인가